Rationale for the Project

Science in modern research departments now generates enormous quantities
of data at an increasingly rapid pace. This is true even in disciplines
such as Psychology where only very recently data might have been
assembled by hand and processed on desktop PCs using standard office
software. Now it is not uncommon for associated neuroimaging centres to
generate huge datasets that require sophisticated techniques to both
manage the data and specialised software to analyse it. The increase in
available raw data shows no sign of decreasing. In parallel, computer
technology has also evolved at an exponential rate. The computers of
today are vastly superior to their predecessors of only twenty years
ago. They are capable of storing far more data and of processing
information much more rapidly. This increase in processing capacity also
shows no sign of decreasing. Unfortunately what has not kept pace with
this revolution in raw data generation and increased capabilities in
processing are the methods, tools and mindset of many of the researchers
who are involved in such research. So rapid has been the revolution in
data acquisition and the expansion of computer power that many
researchers have been left behind and now have a skills gap. We are now
in a time where it is acknowledged that is essential be able to
precisely record all stages of processing data from initial collection
to final publication. This requires capturing all elements of the
scientific process up to and including all statistical tests run. From
this is it should be both possible for other researchers to be able to
understand exactly what has been done by the original researchers, to
duplicate the exact same results, and then to go on, for example, to
explore the consequences of changing the original choices made. There
are modern best practices in reproducible data science that can achieve
this. However, the techniques used by some academic staff and which were
perhaps adequate twenty years ago are no longer fit for purpose in our
modern environment. Worse, because of this information skills gap, such
staff then in turn go on to teach their own junior researchers the same
outdated methods. Some form of structured and freely available
educational framework that would enable researchers and academic at all
levels to catch up with current best practice in reproducible data
science is therefore required.

Aims and Objectives
In this proposal we aim to bring together a collection of world class
experts in the field of reproducible data science. We aim to develop an
initial seed series of workshops and courses that will provide a
knowledge catch up for researchers in reproducible data science
techniques using current methods and state of the art open source
software. We aim to pilot these on our own researchers and educators. We
aim for this group to set up an open source framework for the widest
possible dissemination of this information in a free and easy to access
format. Ultimately, we aim to invite a committed community of
user-contributors (researchers and educators who will edit and add to
the teaching material) who will continue to expand and develop this
framework such that it becomes an acknowledged source of excellence and
long outlives the project itself.

Potential Applications and Benefits
The primary application of this project is to provide an additional but
essential new source of authoritative information regarding current best
practices in data science that it is easily and freely accessible to a
very large number of researchers and educators in research environments.
This will allow individuals to selectively update their own skills base
and in turn teach these practices to their own junior researchers and
research students. The benefit will be the establishment of a world
class and ongoing educational framework and importantly an overall
enhancement of the quality and reproducibility of the work that these
researchers undertake.


